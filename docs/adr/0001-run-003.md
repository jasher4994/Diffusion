# 0002: Optimize Training Configuration for Faster Convergence

Date: 2025-06-09

## Status
Proposed

## Context
After 100 epochs of training, the model showed very slow loss decrease (plateau around 0.06) with images still appearing as pure noise at 32x32 resolution. The original configuration used conservative hyperparameters that led to slow convergence:
- TIMESTEPS = 500 (difficult denoising problem)
- LEARNING_RATE = 5e-6 (very conservative)
- BATCH_SIZE = 4 (small batches, noisy gradients)
- BASE_CHANNELS = 128 (high capacity, slower training)

The model was learning (loss decreasing, different prompts producing different noise patterns) but not efficiently breaking through to recognizable patterns.

## Decision
Implement aggressive optimization focused on faster learning while maintaining 32x32 resolution:

**Denoising Simplification:**
- TIMESTEPS: 500 → 200 (60% easier denoising problem)
- NUM_INFERENCE_STEPS: 50 (maintaining 4x compression ratio)

**Learning Acceleration:**
- LEARNING_RATE: 5e-6 → 5e-5 (10x increase for plateau breaking)
- BATCH_SIZE: 4 → 8 (more stable gradients)

**Model Efficiency:**
- BASE_CHANNELS: 128 → 96 (25% parameter reduction)
- TIME_EMB_DIM: 256 → 192 (computational efficiency)

**Monitoring Enhancement:**
- GENERATE_SAMPLES_EVERY: 10 → 5 (more frequent visual feedback)
- CHECKPOINT_EVERY: 20 → 10 (better progress tracking)

## Alternatives Considered

**Alternative 1: Resolution Scaling**
- Increase IMAGE_SIZE to 64x64
- Rejected: Wanted to isolate hyperparameter effects first

**Alternative 2: Architecture Changes**
- Add attention layers, change UNet structure
- Rejected: Too complex for current debugging phase

**Alternative 3: Conservative Tuning**
- Small learning rate increases (2x instead of 10x)
- Rejected: Previous plateau suggests need for aggressive changes

**Alternative 4: Learning Rate Scheduling**
- Implement cosine annealing or step decay
- Deferred: Will implement if current approach shows promise

## Consequences

**Positive:**
- Faster convergence expected due to easier denoising problem
- More stable training from larger batch sizes
- Reduced computational overhead from smaller model
- Better monitoring of progress with frequent sampling
- Higher learning rate should break through existing plateau

**Negative:**
- Higher learning rate risks training instability
- Smaller model may have reduced representational capacity
- Easier denoising problem may limit final quality ceiling
- Need to retrain from scratch (previous 100 epochs lost)

**Risks:**
- Training may become unstable with 10x learning rate increase
- Model may converge to local minimum faster but at lower quality
- Reduced timesteps may limit fine detail generation capability

**Mitigation:**
- Monitor loss curves closely for instability signs
- Generate samples every 5 epochs to track visual progress
- Prepared to reduce learning rate if oscillation occurs
- Can increase timesteps in future iterations if quality ceiling is reached

**Success Metrics:**
- Visible progress in generated samples by epoch 20
- Loss decrease rate faster than previous 0.01 per 26 epochs
- Different prompts producing clearly distinguishable patterns
- Breakthrough from noise to recognizable blob shapes by epoch 50